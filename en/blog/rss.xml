<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>ùë™ùë≤ùíäùíÉùíÇùíèùíÇ Blog</title>
        <link>http://CKibana.inf.17usoft.com/docs/ckibana-docs/en/blog</link>
        <description>ùë™ùë≤ùíäùíÉùíÇùíèùíÇ Blog</description>
        <lastBuildDate>Tue, 27 Aug 2024 02:54:50 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[How to Build a Logging Platform using Native Kibana and ClickHouse]]></title>
            <link>http://CKibana.inf.17usoft.com/docs/ckibana-docs/en/blog/build-logging-platform-with-native-kibana-and-clickhouse</link>
            <guid>/build-logging-platform-with-native-kibana-and-clickhouse</guid>
            <pubDate>Tue, 27 Aug 2024 02:54:50 GMT</pubDate>
            <description><![CDATA[How to Build a Logging Platform using Native Kibana and ClickHouse]]></description>
            <content:encoded><![CDATA[<blockquote><p>This article will introduce how to build a logging platform based on native Kibana and Clickhouse.</p></blockquote><p>In the rapid development of business, the demand for querying and analyzing various log data leads to a sharp increase in the scale of log storage. Traditional ELK and log systems centered around ElasticSearch subsequently face many challenges in terms of cost, stability, and performance. More and more companies at home and abroad, such as Ctrip, Kuaishou, Bilibili, Cloudflare, and Uber, have been seen switching their storage to ClickHouse, with obvious benefits from their shares. Our log system has also begun to try migrating from ElasticSearch to ClickHouse, exploring and accumulating a set of comprehensive solutions that maximally cater to the existing users&#x27; habits to achieve a smooth transition.</p><h2>1  Background Introduction</h2><p>Since the transition from ElasticSearch to Clickhouse in 2020, our company&#x27;s largest log system has seen significant improvements in both cost and stability. It reliably supported over 500 billion logs per day during this year&#x27;s National Day period, with costs reduced to just 30% of the original ElasticSearch solution.</p><p>Besides this log system, there are many other logging systems within the company, most of which are based on the open-source ELK approach. As scales have increased, issues with costs and stability have gradually emerged, prompting plans to switch all logging system storage foundations to Clickhouse.</p><p>The industry has widely shared the selection and usage of Clickhouse for logging scenarios, which will not be the focus of this article. Those interested can search for relevant information on their own.</p><p>After completing the storage switch, the most crucial issue to address is the query UI user experience. While some companies have developed their own query UIs after switching log storage to Clickhouse, accommodating users&#x27; existing habits for a seamless transition from native Kibana to a new platform is challenging. It requires all business colleagues to familiarize themselves with a new set of syntax and UI interactions, inadvertently adding significant costs.</p><p>Therefore, making it possible for users to migrate to new platform without any learning cost is a rather challenging issue.</p><h2>2 Solution Introduction</h2><p>Our approach is actually quite simple and intuitive. We chose to add an additional proxy layer between native Kibana and Elasticsearch. This proxy is responsible for the syntax translation between Elasticsearch and ClickHouse:
<img src="https://oss.17usoft.com/infra-github/ckibana02.png" alt="Untitled"/></p><p>We developed our own Proxy (named CKibana), which translates chart requests into ClickHouse syntax, fetches results from ClickHouse, and then simulates an Elasticsearch response to return to Kibana. This allows us to directly display data from ClickHouse in the native Kibana interface. In addition to the syntax conversion, we also addressed many practical issues encountered during use.</p><p>Considering the limitations of ClickHouse&#x27;s query concurrency capabilities, we retained Elasticsearch. This Elasticsearch can be used for advanced features such as result caching and storing metadata related to Kibana, and it is very lightweight.</p><h2>3 How to Use CKibana</h2><h3>Components</h3><ol><li>Kibana: Used to provide a UI display for business purposes.</li><li>ElasticSearch: Used for storing Kibana metadata and for query caching among other advanced features.</li><li>ClickHouse: The storage system where the actual log data is stored.</li><li>CKibana: Provides Proxy and other advanced functionalities, enabling users to query ClickHouse data directly on the native Kibana.</li></ol><h3>Getting Started</h3><h4>Launching CKibana</h4><p>To start using CKibana, you&#x27;ll first need to configure it with the necessary Elasticsearch details.</p><p><img src="https://oss.17usoft.com/infra-github/ckibana03.png" alt="Untitled"/></p><p>Once you have your configuration set up, ensure that you have JDK 17 or higher installed on your system for CKibana to run. You can then launch CKibana with the following command:</p><p><code>java -jar ckibana.jar</code></p><h4>Kibana Configuration</h4><p>To modify the Kibana configuration, change the Elasticsearch address to the CKibana addressÔºö
<img src="https://oss.17usoft.com/infra-github/ckibana04.png" alt="Untitled"/></p><p>At this point, Kibana is fully functional and can use CKibana as an Elasticsearch ProxyÔºö
<img src="https://oss.17usoft.com/infra-github/ckibana05.png" alt="Untitled"/></p><h4>Configuring ClickHouse Connection Information and Index Whitelist</h4><p>Set up the ClickHouse connection:</p><p><code>curl --location --request POST &#x27;localhost:8080/config/updateCk?url=ckUrl&amp;user=default&amp;pass=default&amp;defaultCkDatabase=ops&#x27;</code></p><h4>Configure the index to switch to ClickHouse</h4><p><code>curl --location --request POST &#x27;localhost:8080/config/updateWhiteIndexList?list=index1,index2&#x27;</code></p><p>The corresponding relationship between field types in ElasticSearch and ClickHouse is as follows:</p><table><thead><tr><th>es</th><th>ck</th></tr></thead><tbody><tr><td>keyword</td><td>String</td></tr><tr><td>text</td><td>String</td></tr><tr><td>ip</td><td>String(support ipv4 and ipv6)</td></tr><tr><td>integer</td><td>Int32</td></tr><tr><td>long</td><td>Int64</td></tr><tr><td>float</td><td>Float32</td></tr><tr><td>double</td><td>Float64</td></tr></tbody></table><h4>Create index pattern</h4><p><img src="https://oss.17usoft.com/infra-github/ckibana06.png" alt="Untitled"/></p><p>Key Points to Note:</p><ol><li>First, ensure that the input index pattern matches the ClickHouse table exactly; the index pattern and the ClickHouse table name must be an exact match.</li><li>If the corresponding table cannot be selected, you can troubleshoot based on the SQL in the CKibana logs to see if the corresponding table can be queried.</li><li>Pay attention to the time field; otherwise, the time field will not be selectable. The selection logic is as follows:</li></ol><ul><li>Fields of the Date type, such as DateTime64, will be considered as time types.</li><li>Field names containing &quot;time&quot;, for example (@timestamp UInt64), will be considered as time types.</li></ul><p>In either of these two cases, if any one condition is met, the field will be considered a time field. If you are unable to select a time field, it&#x27;s necessary to check whether the fields in the ClickHouse table comply with the matching logic.</p><h4>Here we go</h4><p>After configuring the index pattern, you can now make full use of Kibana&#x27;s visual analysis capabilities<img src="https://oss.17usoft.com/infra-github/ckibana07.png" alt="Untitled"/></p><h3>Advanced Features</h3><h4>Sampling</h4><p>Most of Kibana&#x27;s charts focus on trends. When the result set is too large, it consumes more ClickHouse resources. We provide a sampling feature that ensures the chart trends are close to actual trends while effectively controlling the use of ClickHouse resources, especially when dealing with large datasets.</p><p>Note:</p><ul><li>The corresponding ClickHouse table needs to be created according to the ck sampling table requirements.<a href="https://clickhouse.com/docs/en/sql-reference/statements/select/sample">clickhouse sample</a></li><li>If the sampling threshold is set too low, it can result in a significant difference between the reconstructed values and the true values. We have set our online sampling threshold to 5 million.</li></ul><p>Enabling sampling requires two steps:</p><ol><li>Configure the tables to be sampled.</li><li>Update the sampling threshold. Sampling is triggered when the result set exceeds this threshold.</li></ol><p>Sampling logic: <code>Math.max(0.01, Double.parseDouble(String.format(&quot;%.5f&quot;, sampleParam.getSampleCountMaxThreshold() * 1.00 / sampleParam.getSampleTotalCount())))</code></p><p><img src="https://oss.17usoft.com/infra-github/ckibana08.png" alt="Untitled"/></p><p>Expanding the response from the rate limiter, you can see the sampling value.</p><h4>Time Rounding + Caching</h4><p>When an issue occurs online, a large number of SRE and business colleagues need to query the nginx logs for troubleshooting, and their query conditions are mostly the same. However, ClickHouse aims to achieve the best query performance by utilizing as many CPUs as possible for computations. This situation leads to ClickHouse&#x27;s CPU usage spiking to full capacity. Moreover, under continuous retries by colleagues, the CPU cannot recover.</p><p>Therefore, we implemented a feature for time rounding + caching.</p><p>Time Rounding: For instance, setting rounding to 20s means that the second&#x27;s precision in the query time conditions will be %20, effectively introducing a maximum delay of 20s for data queries.With time rounding in place, a large number of query conditions become identical. At this point, enabling result caching can significantly alleviate the pressure on ClickHouse.</p><p>Set up time rounding:
<code>curl --location --request POST &#x27;localhost:8080/config/updateRoundAbleMinPeriod?roundAbleMinPeriod=20000&#x27; Âçï‰Ωçms</code></p><p>Enable caching:
<code>curl --location --request POST &#x27;localhost:8080/config/updateUseCache?useCache=true&#x27;</code>
<img src="https://oss.17usoft.com/infra-github/ckibana09.png" alt="Untitled"/></p><p>Whether the cache is hit can be seen in the response structure.</p><h4>Query Monitoring + Blacklisting</h4><p>Kibana&#x27;s query syntax is relatively flexible, but some queries can consume substantial resources from ClickHouse. Therefore, we have implemented monitoring for all queries and their execution times. This allows us to easily view which queries have been performed and set up blacklisting controls for them. By doing this, we can restrict queries that are not very efficient.</p><p>Enable monitoring:</p><p><code>curl --location --request POST &#x27;localhost:8080/config/updateEnableMonitoring?enableMonitoring=true&#x27;</code></p><p><img src="https://oss.17usoft.com/infra-github/ckibana10.png" alt="Untitled"/></p><p>As shown in the figure above, we can monitor the details, syntax, and execution time of each query.</p><p><img src="https://oss.17usoft.com/infra-github/ckibana11.png" alt="Untitled"/></p><p>This allows us to leverage Kibana&#x27;s powerful built-in chart features for more intuitive analysis.</p><h4>Query TimeRange Limits</h4><p>Often, when users want to view the latest trends based on certain conditions, they might directly query data for recent periods, such as the last 7 days. This can lead to significant resource consumption. To manage this, CKibana has implemented a maximum time range for queries, which helps in limiting usage and conserving resources.</p><p><code>curl --location --request POST &#x27;localhost:8080/config/updateMaxTimeRange?maxTimeRange=864000000&#x27; Âçï‰Ωçms</code></p><h4>keyword Query</h4><p>To better align with ElasticSearch usage conventions, a <code>field.keyword</code> query is equivalent to an exact search on the field, whereas without .keyword it implies a fuzzy search.</p><p>For example, <code>host.keyword:&quot;www.baidu.com&quot;</code> when translated into SQL becomes:<code>host=&quot;www.baidu.com&quot;</code>.</p><h4>Discover Performance Optimization</h4><p>ClickHouse is highly suitable for analytical processing (AP) scenarios, particularly when large time spans are involved in queries. Traditional SQL commands like <code>select x from table where x order by time desc limit 10</code> can lead to very low query performance and consume a significant amount of ClickHouse resources.</p><p>To address scenarios that involve trend graphs plus details, we have optimized performance to fully leverage ClickHouse&#x27;s AP capabilities. This optimization involves splitting the execution into two steps:</p><ol><li>Leveraging ClickHouse&#x27;s aggregation capabilities to query the number of logs that meet the criteria per minute.</li><li>Based on the number of logs per minute, automatically trimming the log search time span. For instance, if the number of logs within a minute fulfills the requirement, then the query detail time span is automatically reduced to one minute.</li></ol><p>This feature of automatically trimming the query time leads to a significant improvement in the query performance of the Discover version and greatly optimizes CPU usage in ClickHouse.</p><p><img src="https://oss.17usoft.com/infra-github/ckibana15.png" alt="Untitled"/></p><p>As illustrated above, a Discover query is divided into three SQL statements:</p><ol><li>Determine whether sampling is needed</li><li>Count the number of logs per minute</li><li>Automatically trim the query time range</li></ol><h2>4 Usage scenarios: Nginx logs</h2><h4>Clickhouse Table</h4><pre><code class="language-CREATE" metastring="TABLE bjops.ops_bjtlblog_local">(
`@timestamp` UInt64,
`X-Request-Id` String,
`addr` String,
`ap_area` String,
`byte` Int64,
`bytes_recv` Int64,
`Bbtes_sent` Int64,
`content-type` String,
`content_length` Int64,
`crp` String,
`csi` String,
`cspanid` String,
`difftime` Int32,
`error_body` String,
`error_client` String,
`error_host` String,
`error_request` String,
`error_server` String,
`error_upstream` String,
`forwarded` String,
`host` String,
`hostname` String,
`idc` LowCardinality(String),
`index_name` LowCardinality(String),
`ip` String,
`logant_idc` LowCardinality(String),
`logant_type` LowCardinality(String),
`origin_ip` String,
`referer` String,
`remote_port` String,
`request_method` LowCardinality(String),
`request_time` Int64,
`request_uri` String,
`request_url` String,
`scheme` String,
`server_addr` String,
`server_name` String,
`server_port` String,
`server_protocol` String,
`source` String,
`sspanid` String,
`st` String,
`status` Int32,
`timeuse` Float64,
`traceid` String,
`type` String,
`ua` String,
`up_addr` String,
`up_status` Int32,
`upstream_name` String,
`upstream_response_time` Int32,
`worker_pid` String,
`ck_assembly_extension` String,
`bytes_sent` Int64,
INDEX timestamp_index `@timestamp` TYPE minmax GRANULARITY 8192
)
ENGINE = MergeTree
PARTITION BY (toYYYYMMDD(toDateTime(`@timestamp` / 1000, &#x27;Asia/Shanghai&#x27;)), toHour(toDateTime(`@timestamp` / 1000, &#x27;Asia/Shanghai&#x27;)))
ORDER BY (host, request_uri, intHash64(`@timestamp`))
SAMPLE BY intHash64(`@timestamp`)
SETTINGS in_memory_parts_enable_wal = 0, index_granularity = 8192
</code></pre><p>Always place <code>host</code> at the first position in the ORDER BY clause, as most Nginx log queries require sorting based on the <code>host</code>.</p><h4>CKibana Configuration</h4><pre><code>{
    &quot;Proxy&quot;: {
        &quot;ck&quot;: {
            &quot;url&quot;: &quot;ip:6321&quot;,
            &quot;user&quot;: &quot;user&quot;,
            &quot;pass&quot;: &quot;pass&quot;,
            &quot;defaultCkDatabase&quot;: &quot;db&quot;
        },
        &quot;es&quot;: {
            &quot;host&quot;: &quot;ip:31940&quot;
        },
        &quot;roundAbleMinPeriod&quot;: 120000,
        &quot;round&quot;: 20000,
        &quot;maxTimeRange&quot;: 86400000,
        &quot;blackIndexList&quot;: null,
        &quot;whiteIndexList&quot;: [&quot;ops_bjtlblog_all&quot;, &quot;other_index_all&quot;],
        &quot;enableMonitoring&quot;: true
    },
    &quot;query&quot;: {
        &quot;sampleIndexPatterns&quot;: [&quot;ops_bjtlblog_all&quot;],
        &quot;sampleCountMaxThreshold&quot;: 5000000,
        &quot;useCache&quot;: true,
        &quot;maxResultRow&quot;: 30000
    },
    &quot;threadPool&quot;: {
        &quot;msearchProperty&quot;: {
            &quot;coreSize&quot;: 4,
            &quot;queueSize&quot;: 10000
        },
        &quot;commonProperty&quot;: {
            &quot;coreSize&quot;: 4,
            &quot;queueSize&quot;: 10000
        }
    },
    &quot;defaultShard&quot;: 2
}
</code></pre><h4>Use Cases</h4><p><img src="https://oss.17usoft.com/infra-github/ckibana12.png" alt="Untitled"/></p><p><img src="https://oss.17usoft.com/infra-github/ckibana13.png" alt="Untitled"/></p><h2>4 Benefits</h2><p>As of now, by leveraging CKibana&#x27;s core capabilities, we have successfully completed the full migration of all Nginx access logs and business-customized logs from Elasticsearch to Clickhouse, reducing our storage costs to below 30% of the original. Moreover, thanks to ClickHouse&#x27;s distributed table capabilities, log queries remain unaffected even in the event of a single center failure, offering significant improvements over Elasticsearch in both cost and stability. We have also continued to use the flexible and powerful native Kibana as our visualization tool, allowing users to conveniently perform log queries and analyses using their familiar Kibana dashboard.</p><h2>5 Finally</h2><p>Our logging system&#x27;s continuous evolution owes much to numerous outstanding open-source projects. Having proven stable and reliable internally, we&#x27;re now excited to open-source the CKibana project. We also look forward to collaborating with the community to continuously enhance its functionality, fully leveraging the Kibana visualization and ClickHouse storage combination for log analysis.</p><p>Github Repo: <a href="https://github.com/TongchengOpenSource/ckibana/">https://github.com/TongchengOpenSource/ckibana/</a></p><p>We hope you find it useful and welcome your feedback. Thanks~</p>]]></content:encoded>
        </item>
    </channel>
</rss>